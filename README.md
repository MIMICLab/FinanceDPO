# FinanceDPO

**Direct Preference Optimization for Decision-Driven Financial Time-Series Forecasting**

---

## âœ¨ Whatâ€™s inside?

* **Opinionated Python project layout** â€“ `src/`, `notebooks/`, `tests/`, `docs/`
* **Ready-to-run training script** â€“ [`src/train.py`](src/train.py) wired for **Hydra** + **PyTorch Lightning**
* **Conda environment spec** â€“ [`environment.yml`](environment.yml) for reproducible installs
* **CI & style tooling** â€“ `.pre-commit-config.yaml`, `.gitignore`, GitHub Actions, `pytest` placeholder
* **Developer docs** â€“ this `README.md`, [`CONTRIBUTING.md`](CONTRIBUTING.md), MIT [`LICENSE`](LICENSE)

---

## ğŸ”§ Quick start

### 1. Clone & set up
```bash
# Clone
git clone https://github.com/MIMICLab/FinanceDPO.git
cd FinanceDPO

# Create environment
conda env create -f environment.yml
conda activate dpo-finance

> **Python version**: `environment.yml` pins `python=3.11`.  
> If your base conda uses a different version, run  
> `conda env create -f environment.yml -n dpo-finance`.
```

### 2. Download raw price data (example: S&P 500)
```bash
python src/dpo_forecasting/data/download.py \
    --symbols-file data/sp500.txt \
    --start 2000-01-01
```
> `data/sp500.txt` is a plain-text list of ticker symbols, one per line (e.g. `AAPL`, `MSFT`, â€¦).

### 3. Generate preference pairs
```bash
python src/dpo_forecasting/data/make_pairs.py \
    --prices-dir data/raw \
    --lookahead 7  --lookback 365 \
    --good-quantile 0.8 --bad-quantile 0.2 \
    --cache-file data/pairs_cache.pt \
    --skip-parquet             # (ì„ íƒ) Parquet ê±´ë„ˆë›°ê¸°
```
> If you supply `--skip-parquet`, only the Torch cache (`pairs_cache.pt`) is written.  
> Keep the `--skip-parquet` flag **off** if you still want a `.parquet` file for evaluation utilities that expect it.

### 4. Train a DPO model
```bash
python src/train.py \
   dataset.cache_file=data/pairs_cache.pt \
   trainer.max_epochs=30
```

### 5. Evaluate & back-test
```bash
# 5â€‘a. Pairwise accuracy / ROC
python src/eval.py --checkpoint latest.ckpt \
                   --pairs-file data/pairs.parquet     # or --cache-file

# 5â€‘b. Quick daily long/short strategy
python src/backtest.py --checkpoint latest.ckpt \
                       --prices-dir data/raw

# 5â€‘c. Advanced backâ€‘test (position sizing, stopâ€‘loss, leverage, cost)
python src/advanced_backtest.py --checkpoint latest.ckpt \
                                --prices-dir data/raw \
                                --lookback 31 --hold 5 \
                                --score-thresh 0.5 \
                                --max-long 0.3 --max-short 0.1 \
                                --cost-bps 5
```
`advanced_backtest.py` applies positionâ€‘scaled weights, transaction costs,
and optional equity stops, giving a closer approximation to live
portfolio behaviour.  Adjust `--hold`, `--max-long`, `--cost-bps` and other
flags to reflect your trading assumptions.

### 6. Fineâ€‘tune a single symbol with a frozen referenceÂ net

You can first train a **broad, marketâ€‘wide model** (e.g. on all S&Pâ€¯500 pairs),
then fineâ€‘tune it for a specific symbol while keeping the original behaviour
as a reference via a small KLÂ penalty:

```bash
# 7â€‘a. Train a base model on the full S&PÂ 500 cache
python src/train.py \
    dataset.cache_file=data/sp500_pairs_cache.pt \
    trainer.max_epochs=20 \
    +train.kl_coeff=0           # KL off

# 7â€‘b. Fineâ€‘tune on AAPL only, pulling against the frozen base net
python src/train.py \
    dataset.cache_file=data/aapl_pairs_cache.pt \
    +model.init_from=runs/sp500_base.ckpt \
    +train.kl_coeff=0.05        # small KL to retain global knowledge
```

The first command produces `runs/sp500_base.ckpt`.  
The second command loads that checkpoint asâ€¯`reference_net`, freezes it
internally, and trains a new policy that specializes on AAPL while being softly
regularized toward the base model.

### 7. Hyperparameter optimizationâ€¯â€”â€¯endâ€‘toâ€‘end (pairsÂ +Â training)

The fileÂ `configs/hpo.yaml` defines a search space for **lookback / lookahead**,
model size, learning rate, etc.  
A tiny pipeline script regenerates preference pairs **and** trains the model for
each trial, so you can safely vary `dataset.lookback` and `lookahead`:

```bash
python -m src.hpo_pipeline -m +hpo=default
```

* `-m` asks **Hydra** to launch a **multirun** (one process per Optuna trial).  
* The pipeline calls  
 Â 1)Â `data.make_pairs`Â â†’Â creates a unique cache file  
 Â 2)Â `train.py`Â â†’Â trains on that cache and prints `val_sharpe`  
* Optuna maximizes the first metric logged with `prog_bar=True`; we recommend
Â Â `val_sharpe`.

Results are stored under `multirun/YYYY-MM-DD/HH-MM-SS/{0,1,2,â€¦}/`.  
Open `optuna.log` there to see the best hyperâ€‘parameters.

> **Tipâ€‚:** edit `configs/hpo.yaml` to add or widen ranges, e.g.  
> `dataset.lookback: "choice(21, 31, 61, 91)"`.
  


---

## ğŸ“‚ Project layout
```
FinanceDPO/
â”œâ”€â”€ data/                  # raw & processed data (git-ignored)
â”œâ”€â”€ docs/                  # extended docs / diagrams
â”œâ”€â”€ notebooks/             # exploratory notebooks
â”œâ”€â”€ src/
â”‚Â Â  â””â”€â”€ dpo_forecasting/
â”‚Â Â      â”œâ”€â”€ data/          # data loaders & pair builders
â”‚Â Â      â”œâ”€â”€ models/        # model definitions & loss functions
â”‚Â Â      â””â”€â”€ configs/       # Hydra configs (dpo.yaml, hpo.yaml â€¦)
â”œâ”€â”€ tests/                 # unit tests
â”œâ”€â”€ environment.yml        # conda spec
â”œâ”€â”€ pyproject.toml         # install package via `pip install -e .`
â”œâ”€â”€ .github/               # CI workflows
â””â”€â”€ README.md
```

---

## ğŸ§‘â€ğŸ’» Contributing

We welcome pull requests! See [`CONTRIBUTING.md`](CONTRIBUTING.md) for the workflow.

---

## ğŸ“œ License

This project is licensed under the MIT License. See the [`LICENSE`](LICENSE) file for details.
